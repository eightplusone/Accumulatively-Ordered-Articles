{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load seed article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Alli and Kane June 12 – Neymar has the highest transfer value in the world at 210.7 million, according to latest data. Tottenham's Dele Alli is second placed, 55 million less than Neymar at 155.1 million, but worth 1.5 million than third placed teammate Harry Kane. Both are ahead of Lionel Messi who is valued at 151.7 million. Messi topped the list in 2016 with a 211.1 million transfer valuation.  The new CIES Football Observatory data is the 2017 summer transfer valuations for players in Europe's Big-5 leagues. CIES have ranked the top 110 players with an estimated value of more than 40 million.  The English Premier League not surprisingly has the most players in the list with 13 of the top 25 players and 23 of the top 50. There are 10 English born players in the top 110  Report authors highlight \"the estimated transfer value of the tenth most expensive Premier League player is 85 million. This figure is 73 million for the Spanish Liga, 51 million for the Italian Serie A, 47 million for the German Bundesliga and 40 million for the French Ligue 1. The youngest player in the rankings is the French prodigy Kylian Mbappe (93 million – 18th), while the oldest is the evergreen Portuguese striker Cristiano Ronaldo (112 million – 11th).\"  CIES Football Observatory have developed their own algorithm for calculating transfer value based on the analysis of almost 2,000 paying fee transfers. The variables used for the calculation include the performance of both players and employer clubs, international status, contract, age and position.\n"
     ]
    }
   ],
   "source": [
    "# Seed article\n",
    "seed = \"data/17-5-5\"\n",
    "seed_date = seed[5:].split(\"-\")\n",
    "\n",
    "with open(seed, 'r') as seedfile:\n",
    "    seeddata = seedfile.read().replace('\\n', ' ')\n",
    "    seeddata = seeddata.replace('£', '')\n",
    "    seeddata = seeddata.replace('€', '')\n",
    "    seeddata = seeddata.replace('@', '')\n",
    "    \n",
    "print(seeddata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract chunks of candidate keyphrases\n",
    "\n",
    "Reference: http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/\n",
    "\n",
    "Note: We only get chunks that follow this pattern: (&lt;JJ&gt;\\* &lt;NN.\\*&gt;+ &lt;IN&gt;)? &lt;JJ&gt;\\* &lt;NN.\\*&gt;+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def keyword_extraction(text, grammar = r\"KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}\"):\n",
    "    import itertools, nltk, string\n",
    "    \n",
    "    # exclude candidates that are stop words or entirely punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    # tokenize, POS-tag, and chunk using regular expressions\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    tagged_sents = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))\n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent)) for tagged_sent in tagged_sents))\n",
    "    \n",
    "    # join constituent chunk words into a single chunked phrase\n",
    "    candidates = [' '.join(word for word, pos, chunk in group).lower() for key, group in itertools.groupby(all_chunks, lambda (word,pos,chunk): chunk != 'O') if key]\n",
    "    # Remove duplicates\n",
    "    candidates = list(set(candidates))\n",
    "    # Sort\n",
    "    candidates.sort()\n",
    "    \n",
    "    #print(\"Number of candidate chunks: \" + str(len(candidates)))\n",
    "    return [cand for cand in candidates if cand not in stop_words and not all(char in punct for char in cand)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract key chunks of the seed article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'alli', 'analysis', 'big-5 leagues', 'calculation', 'cies', 'cies football observatory', 'contract', 'data', 'dele alli', 'employer clubs', 'english premier league', 'europe', 'evergreen portuguese striker cristiano ronaldo', 'expensive premier league player', 'fee transfers', 'figure', 'french ligue', 'french prodigy kylian mbappe', 'german bundesliga', 'international status', 'italian serie a', 'kane june', 'lionel messi', 'list', 'messi', 'new cies football observatory data', 'neymar', 'own algorithm', 'performance', 'player', 'players', 'position', 'rankings', 'report authors', 'spanish liga', 'summer transfer valuations for players', 'tenth', 'third placed teammate harry kane', 'tottenham', 'transfer valuation', 'transfer value', 'value', 'variables', 'world', '\\xe2\\x80\\x93 neymar']\n"
     ]
    }
   ],
   "source": [
    "seed_keychunks = keyword_extraction(seeddata)\n",
    "print(seed_keychunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extract key chunks from the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/17-4-20\n",
      "data/17-4-24\n",
      "data/17-4-27\n",
      "data/17-5-1\n",
      "data/17-5-10\n",
      "data/17-5-19\n",
      "data/17-5-24\n",
      "data/17-5-5\n",
      "data/17-7-1\n",
      "data/17-7-1-2\n",
      "data/17-7-13\n",
      "data/17-7-18\n",
      "data/17-7-18-2\n",
      "data/17-7-18-3\n",
      "data/17-7-18-4\n",
      "data/17-7-19\n",
      "data/17-7-19-2\n",
      "data/17-7-19-3\n",
      "data/17-7-19-4\n",
      "data/17-7-19-5\n",
      "data/17-7-24\n",
      "data/17-7-24-2\n",
      "data/17-7-24-3\n",
      "data/17-7-28\n",
      "data/17-7-30\n",
      "data/17-7-30-2\n",
      "data/17-7-30-3\n",
      "data/17-7-31\n",
      "data/17-8-1\n",
      "data/17-8-17\n",
      "data/17-8-2\n",
      "data/17-8-2-2\n",
      "data/17-8-2-3\n",
      "data/17-8-28\n",
      "data/17-8-3\n",
      "data/17-8-3-2\n",
      "data/17-8-31\n",
      "data/17-8-4\n",
      "data/17-8-4-2\n",
      "data/17-9-1\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "path = \"data/*\"   \n",
    "files = glob.glob(path)\n",
    "all_articles = []\n",
    "all_keychunks = []\n",
    "all_dates = []\n",
    "\n",
    "for file in files:\n",
    "    if not (file == seed):\n",
    "        print(file)\n",
    "        \n",
    "        # Get the date of the article\n",
    "        date = file[5:].split(\"-\")\n",
    "        all_dates.append(date)\n",
    "\n",
    "        # Read content\n",
    "        f = open(file, \"r\")\n",
    "        fdata = f.read().replace('\\n', ' ')\n",
    "        fdata = fdata.replace('£', '')\n",
    "        fdata = fdata.replace('€', '')\n",
    "        fdata = fdata.replace('@', '')\n",
    "        all_articles.append([file, fdata])\n",
    "        all_keychunks.append(keyword_extraction(fdata))\n",
    "        f.close()\n",
    "        \n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['17', '4', '20'], ['17', '4', '24'], ['17', '4', '27'], ['17', '5', '1'], ['17', '5', '10'], ['17', '5', '19'], ['17', '5', '24'], ['17', '5', '5'], ['17', '7', '1'], ['17', '7', '1', '2'], ['17', '7', '13'], ['17', '7', '18'], ['17', '7', '18', '2'], ['17', '7', '18', '3'], ['17', '7', '18', '4'], ['17', '7', '19'], ['17', '7', '19', '2'], ['17', '7', '19', '3'], ['17', '7', '19', '4'], ['17', '7', '19', '5'], ['17', '7', '24'], ['17', '7', '24', '2'], ['17', '7', '24', '3'], ['17', '7', '28'], ['17', '7', '30'], ['17', '7', '30', '2'], ['17', '7', '30', '3'], ['17', '7', '31'], ['17', '8', '1'], ['17', '8', '17'], ['17', '8', '2'], ['17', '8', '2', '2'], ['17', '8', '2', '3'], ['17', '8', '28'], ['17', '8', '3'], ['17', '8', '3', '2'], ['17', '8', '31'], ['17', '8', '4'], ['17', '8', '4', '2'], ['17', '9', '1']]\n"
     ]
    }
   ],
   "source": [
    "print(all_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map all_keychunks to the vocabulary created from seed_keychunks to have one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "one_hot_vectors = []\n",
    "\n",
    "for keychunk in all_keychunks:\n",
    "    vect = []\n",
    "    for chunk in seed_keychunks:\n",
    "        if chunk in keychunk:\n",
    "            vect.append(1)\n",
    "        else:\n",
    "            vect.append(0)\n",
    "    one_hot_vectors.append(vect)\n",
    "\n",
    "for vect in one_hot_vectors:\n",
    "    print(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25537696  0.20851441  0.20851441  0.20851441  0.44232587  0.29488391\n",
      "  0.20851441  0.20851441  0.25537696  0.32969024  0.39009475  0.14744196\n",
      "  0.29488391  0.32969024  0.20851441  0.25537696  0.29488391  0.29488391\n",
      "  0.20851441  0.29488391  0.25537696  0.29488391  0.20851441  0.25537696\n",
      "  0.20851441  0.25537696  0.14744196  0.25537696  0.39009475  0.29488391\n",
      "  0.39009475  0.36115756  0.29488391  0.39009475  0.25537696  0.32969024\n",
      "  0.32969024  0.32969024  0.29488391  0.20851441]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "seed_one_hot = np.array([1] * len(one_hot_vectors[0]))\n",
    "one_hot_vectors = [np.array(vector) for vector in one_hot_vectors]\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "# Cosine similarity\n",
    "one_hot_distance = np.array([1 - spatial.distance.cosine(seed_one_hot, vector) for vector in one_hot_vectors])\n",
    "print(one_hot_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out articles that are published after the seed article's date or have one-hot vector too far away from the seed vector or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "is_selected = [0] * len(one_hot_distance)\n",
    "distance_threshold = 0.2\n",
    "\n",
    "for i in range(len(one_hot_distance)):\n",
    "    if one_hot_distance[i] <= distance_threshold:\n",
    "        if all_dates[i][0] > seed_date[0]:\n",
    "            is_selected[i] = 1\n",
    "        elif all_dates[i][0] == seed_date[0] and all_dates[i][1] > seed_date[1]:\n",
    "            is_selected[i] = 1\n",
    "        elif all_dates[i][0] == seed_date[0] and all_dates[i][1] == seed_date[1] and all_dates[i][2] >= seed_date[2]:\n",
    "            is_selected[i] = 1\n",
    "\n",
    "print(is_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compare(article_index_1, article_index_2):\n",
    "    score = 0.0\n",
    "    \n",
    "    # Weights\n",
    "    w_year = 1.0\n",
    "    w_month = 1.0\n",
    "    w_day = 1.0\n",
    "    w_vector_distance = 1.0\n",
    "    \n",
    "    # Process the date\n",
    "    #date = [all_articles[article_index_1][0][5:], all_articles[article_index_2][0][5:]]\n",
    "    #date = [d.split(\"-\") for d in date]\n",
    "    \n",
    "    score += ((float(all_dates[article_index_1][0]) - float(all_dates[article_index_2][0])) * w_year)\n",
    "    score += ((float(all_dates[article_index_1][1]) - float(all_dates[article_index_2][1])) * w_month)\n",
    "    score += ((float(all_dates[article_index_1][2]) - float(all_dates[article_index_2][2])) * w_day)\n",
    "    score += ((one_hot_distance[article_index_1] - one_hot_distance[article_index_2]) * w_vector_distance)\n",
    "\n",
    "    return score >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(38, 39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "[0, 1, 3, 2, 4, 5, 6, 8, 7, 9, 10, 11, 12, 13, 14, 15, 18, 17, 16, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 30, 29, 32, 33, 34, 35, 39, 38, 37, 36]\n",
      "['data/17-4-20', 'data/17-4-24', 'data/17-4-27', 'data/17-5-1', 'data/17-5-10', 'data/17-5-19', 'data/17-5-24', 'data/17-5-5', 'data/17-7-1', 'data/17-7-1-2', 'data/17-7-13', 'data/17-7-18', 'data/17-7-18-2', 'data/17-7-18-3', 'data/17-7-18-4', 'data/17-7-19', 'data/17-7-19-2', 'data/17-7-19-3', 'data/17-7-19-4', 'data/17-7-19-5', 'data/17-7-24', 'data/17-7-24-2', 'data/17-7-24-3', 'data/17-7-28', 'data/17-7-30', 'data/17-7-30-2', 'data/17-7-30-3', 'data/17-7-31', 'data/17-8-1', 'data/17-8-17', 'data/17-8-2', 'data/17-8-2-2', 'data/17-8-2-3', 'data/17-8-28', 'data/17-8-3', 'data/17-8-3-2', 'data/17-8-31', 'data/17-8-4', 'data/17-8-4-2', 'data/17-9-1']\n"
     ]
    }
   ],
   "source": [
    "sorted_article_indices = [i for i in range(len(all_articles))]\n",
    "print(sorted_article_indices)\n",
    "\n",
    "for i in range(len(all_articles)):\n",
    "    for j in range(len(all_articles) - i - 1):\n",
    "        if (compare(j, j+1)):\n",
    "            sorted_article_indices[j], sorted_article_indices[j+1] = sorted_article_indices[j+1], sorted_article_indices[j]\n",
    "\n",
    "print(sorted_article_indices)\n",
    "print([all_articles[sorted_article_indices[i]][0] for i in sorted_article_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for additional topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article data/17-7-18\n",
      "How many topics will be added: 30\n",
      "Article data/17-7-30-3\n",
      "How many topics will be added: 54\n"
     ]
    }
   ],
   "source": [
    "accum_topics = seed_keychunks\n",
    "\n",
    "for i in range(len(sorted_article_indices)):\n",
    "    if is_selected[i] == 1:\n",
    "        print(\"Article \" + str(all_articles[sorted_article_indices[i]][0]) + \"\\nHow many topics will be added: \" + str(len(set(all_keychunks[sorted_article_indices[i]]) - set(accum_topics))))\n",
    "        accum_topics = list(set(accum_topics) | set(all_keychunks[sorted_article_indices[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NaturalLanguageUnderstandingV1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-d2b90da17853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m natural_language_understanding = NaturalLanguageUnderstandingV1(\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0musername\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fd9d2f6e-194a-4872-bc99-4d7b617b5e74'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'WnJkJKOk2bIw'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   version='2017-02-27')\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#\"url\": \"https://gateway.watsonplatform.net/natural-language-understanding/api\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NaturalLanguageUnderstandingV1' is not defined"
     ]
    }
   ],
   "source": [
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "  username='fd9d2f6e-194a-4872-bc99-4d7b617b5e74',\n",
    "  password='WnJkJKOk2bIw',\n",
    "  version='2017-02-27')\n",
    "#\"url\": \"https://gateway.watsonplatform.net/natural-language-understanding/api\",\n",
    "#  \"username\": \"fd9d2f6e-194a-4872-bc99-4d7b617b5e74\",\n",
    "#  \"password\": \"WnJkJKOk2bIw\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
